<!DOCTYPE html>
<html lang="pt-br">
<head>
  <meta charset="UTF-8">
  <title>Resumo ‚Äì Redes Neurais e Deep Learning</title>
  <style>
    body {
      font-family: 'Segoe UI', sans-serif;
      background-color: #f8f9fa;
      margin: 30px;
      color: #2c2c2c;
    }
    h1, h2 {
      color: #003366;
    }
    h1 {
      border-bottom: 2px solid #003366;
      padding-bottom: 6px;
    }
    ul {
      margin-top: 0;
      padding-left: 20px;
    }
    .destaque {
      background-color: #e6f0ff;
      padding: 12px;
      border-left: 5px solid #005cbf;
      margin: 20px 0;
    }
    code {
      background-color: #eee;
      padding: 2px 5px;
      border-radius: 3px;
      font-family: "Courier New", monospace;
    }
    table {
      width: 100%;
      border-collapse: collapse;
      margin-top: 15px;
    }
    th, td {
      border: 1px solid #ccc;
      padding: 10px;
    }
    th {
      background-color: #d0e6ff;
    }
  </style>
</head>
<body>

<h1>Intelig√™ncia Artificial ‚Äì Redes Neurais e Deep Learning</h1>

<div class="destaque">
  <strong>üéØ CESPE em foco:</strong> Cobra <em>conceitos estruturais</em> de redes neurais, funcionamento das camadas, fun√ß√£o de ativa√ß√£o, retropropaga√ß√£o e aplica√ß√µes pr√°ticas do deep learning. Quest√µes podem envolver pseudoc√≥digo, interpreta√ß√£o de arquitetura e funcionamento do treinamento.
</div>

<h2>1. Conceitos Fundamentais</h2>
<ul>
  <li><strong>Rede Neural Artificial (RNA):</strong> modelo computacional inspirado no c√©rebro humano.</li>
  <li><strong>Neur√¥nio artificial:</strong> recebe entradas, aplica pesos, soma, aplica fun√ß√£o de ativa√ß√£o e gera uma sa√≠da.</li>
  <li><strong>Camadas:</strong> entrada, escondidas (ocultas), e sa√≠da.</li>
</ul>

<h2>2. Componentes de uma Rede Neural</h2>
<ul>
  <li><strong>Pesos (weights):</strong> determinam a influ√™ncia de cada entrada.</li>
  <li><strong>Bias (vi√©s):</strong> valor adicional somado √† entrada ponderada, permitindo maior flexibilidade.</li>
  <li><strong>Fun√ß√µes de ativa√ß√£o:</strong>
    <ul>
      <li><code>ReLU</code> (mais usada no deep learning): f(x) = max(0, x)</li>
      <li><code>Sigmoid</code>: sa√≠da entre 0 e 1 (problemas bin√°rios).</li>
      <li><code>Softmax</code>: usada na camada de sa√≠da para classifica√ß√£o multiclasse.</li>
    </ul>
  </li>
</ul>

<h2>3. Aprendizado e Treinamento</h2>
<ul>
  <li><strong>Forward propagation:</strong> c√°lculo da sa√≠da a partir das entradas.</li>
  <li><strong>Loss function:</strong> mede o erro da predi√ß√£o (ex: MSE, cross-entropy).</li>
  <li><strong>Backpropagation:</strong> ajusta os pesos com base no erro (gradiente descendente).</li>
  <li><strong>√âpoca (epoch):</strong> uma varredura completa no conjunto de treinamento.</li>
  <li><strong>Lote (batch):</strong> subconjunto dos dados usados em uma atualiza√ß√£o.</li>
</ul>

<h2>4. Deep Learning (Aprendizado Profundo)</h2>
<ul>
  <li>Utiliza <strong>redes neurais profundas</strong> com v√°rias camadas ocultas.</li>
  <li>Requer grande volume de dados e poder computacional.</li>
  <li><strong>Arquiteturas comuns:</strong>
    <ul>
      <li><strong>MLP (Perceptron Multicamadas):</strong> rede totalmente conectada.</li>
      <li><strong>CNN (Redes Convolucionais):</strong> processamento de imagens.</li>
      <li><strong>RNN (Redes Recorrentes):</strong> processamento sequencial (texto, s√©ries temporais).</li>
      <li><strong>LSTM/GRU:</strong> varia√ß√µes de RNN que resolvem problema de longas depend√™ncias.</li>
    </ul>
  </li>
</ul>

<h2>5. Comparativo R√°pido</h2>
<table>
  <thead>
    <tr>
      <th>Tipo de Rede</th>
      <th>Usada para</th>
      <th>Destaque</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>MLP</td>
      <td>Problemas tabulares, classifica√ß√£o b√°sica</td>
      <td>Redes densamente conectadas</td>
    </tr>
    <tr>
      <td>CNN</td>
      <td>Reconhecimento de imagem, v√≠deo</td>
      <td>Usa convolu√ß√µes e pooling</td>
    </tr>
    <tr>
      <td>RNN</td>
      <td>Texto, fala, previs√£o de s√©ries</td>
      <td>Tem mem√≥ria de estados anteriores</td>
    </tr>
    <tr>
      <td>LSTM/GRU</td>
      <td>Tradu√ß√£o autom√°tica, gera√ß√£o de texto</td>
      <td>Controla fluxo de mem√≥ria</td>
    </tr>
  </tbody>
</table>

<h2>6. Pegadinhas CESPE</h2>
<ul>
  <li>Dizer que <strong>todas as camadas em uma rede profunda s√£o totalmente conectadas</strong> ‚Üí <strong>falso</strong> (ex: CNNs usam filtros).</li>
  <li>Confundir <strong>fun√ß√£o de ativa√ß√£o com fun√ß√£o de perda</strong>.</li>
  <li>Afirmar que o <strong>backpropagation ocorre apenas na √∫ltima camada</strong> ‚Üí <strong>falso</strong>, ele ocorre em todas.</li>
  <li>Relacionar <strong>sigmoid com redes profundas</strong> ‚Üí √© usado, mas pode causar problema de vanishing gradient.</li>
</ul>

<div class="destaque">
  <strong>üìå Dica de memoriza√ß√£o:</strong><br>
  <code>Entrada ‚Üí Camadas Ocultas (com ReLU) ‚Üí Sa√≠da (com Softmax) ‚Üí Erro ‚Üí Backpropagation</code><br>
  Treinamento = ajustar pesos para minimizar erro.
</div>

<div class="destaque">
  <strong>‚úîÔ∏è Lembre-se:</strong>
  <ul>
    <li><strong>Deep Learning</strong> = muitas camadas + muitos dados + muito poder de processamento.</li>
    <li>Treinamento supervisionado = dados com r√≥tulos (labels).</li>
    <li>Aprendizado profundo √© um subconjunto de aprendizado de m√°quina.</li>
  </ul>
</div>

</body>
</html>
